{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The purpose of this project is to create a dataset to ease query for jobs. For that we don't need normalized schema, instead star schema is created because of it's scalability in terms of reading and querying. This star schema can be further modified to create more summarized data such as OLAP cubes for BI and analytics (not in scope of this project). Here are few examples of kind of analysis that can be done on the final data to help answer patterns such as below:\n",
    "\n",
    "- How many visas were issued overall or over a period of time? *(dim: visa, dim: date, fact: immigration records)*\n",
    "- What mode immigrants used to travel the most? *(dim: mode, fact: immigration records)*\n",
    "- Which state/port received most no. of non-immigrants during a period of time? *(dim: state, dim: airport, fact immigration records)*\n",
    "- What kind of occupation for a specific visa immigrants had? *(dim: non_immigrant, fact: immigration records)*\n",
    "- Metric w.r.t date like the number of immigrants entered in a year, quarter, month, week? *(dim: date, fact: immigration record)*\n",
    "\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import pandas as d\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('CapstoneProject/conf/config.cfg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3\")\\\n",
    "                     .getOrCreate()\n",
    "spark._jsc.hadoopConfiguration().set(\n",
    "            'fs.s3a.impl', 'org.apache.hadoop.fs.s3native.NativeS3FileSystem')\n",
    "spark._jsc.hadoopConfiguration().set(\n",
    "            'fs.s3a.awsAccessKeyId', config['AWS']['access_key_id'])\n",
    "spark._jsc.hadoopConfiguration().set(\n",
    "            'fs.s3a.awsSecretAccessKey', config['AWS']['secret_access_key'])\n",
    "\n",
    "log4jLogger = spark._jvm.org.apache.log4j\n",
    "log = log4jLogger.LogManager.getLogger(__name__)\n",
    "log.setLevel(log4jLogger.Level.WARN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Scope \n",
    "The scope of this project is to analyze the raw data for immigration, airlines, states provided/gathered from the sources mentioned below. Transform and clean the data to load into schema on read in s3. This data once transformed can be further used to create summary tables/df for analytics.\n",
    "\n",
    "### Describe and Gather Data \n",
    "##### I94 Immigration Data: \n",
    "This data comes from the [US National Tourism and Trade Office](https://www.trade.gov/national-travel-and-tourism-office).\n",
    "The data is provided in both sas and parquet format. We would be using parquet format for this project. This data contains event information related to immigrants. Their entry date, visa, mode of entry, country of origin/residence, their return date, airlines, modes of travel etc. All the event related information is provided in the data files and are already stored in s3.<br><br>\n",
    "##### U.S. City Demographic Data: \n",
    "This data comes from [OpenSoft](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/), file is delimited by semicolon contains information related to city, state, male/female population, racial population and other demographic information. <br><br>\n",
    "##### Airport Code Table: \n",
    "This is a simple table of airport codes and corresponding cities. It comes from [datahub.io](https://datahub.io/core/airport-codes#data). Data is stored in CSV format.<br><br>\n",
    "##### Other Data Sources:\n",
    "Following data sources are coming from I94_SAS_Labels_Descriptions.SAS file transformed into CSV<br>\n",
    "- Countries : Contains country code and name\n",
    "- States : Contains state code and name\n",
    "- Visa : Contain Visa code and type\n",
    "- Mode : Contains mode code and mode name\n",
    "- Ports : Contains port code and port name/state\n",
    "\n",
    "\n",
    "<br>Here is the how the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_data=\"s3a://capstoneprojectsource/sas_data\"\n",
    "visa_data=\"s3a://capstoneprojectsource/source/visa.csv\"\n",
    "mode_data=\"s3a://capstoneprojectsource/source/mode.csv\"\n",
    "country_data=\"s3a://capstoneprojectsource/source/countries.csv\"\n",
    "states_data=\"s3a://capstoneprojectsource/source/states.csv\"\n",
    "ports_data=\"s3a://capstoneprojectsource/source/all_ports.csv\"\n",
    "airport_data=\"s3a://capstoneprojectsource/source/airport-codes_csv.csv\"\n",
    "city_data=\"s3a://capstoneprojectsource/source/us-cities-demographics.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_data_df = spark.read.load(immigration_data, header=True)\n",
    "immigration_data_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- visa_desc: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visa_data_df = spark.read.csv(visa_data, header=True)\n",
    "visa_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mode_data_df = spark.read.csv(mode_data, header=True)\n",
    "mode_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_data_df = spark.read.csv(country_data, header=True)\n",
    "country_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "states_data_df = spark.read.csv(states_data, header=True)\n",
    "states_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- ports: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ports_data_df = spark.read.option(\"delimiter\", \";\").csv(ports_data, header=True)\n",
    "ports_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_data_df = spark.read.csv(airport_data, header=True)\n",
    "airport_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_data_df = spark.read.option(\"delimiter\", \";\").csv(city_data, header=True)\n",
    "city_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I94 Immigration data :\n",
    "- The dataset contains event data for the individuals that entered the united states. The information contains their year of birth, date of arrival,  departure, visa, country of origin or residence, gender, airline, mode/port of entrance.\n",
    "- Information that is not related to event but are related to immigrant can be put in a dimension table like year of birth, occupation, residence of immigrant etc. This will help any kind of analytics to be performed on immigrant type/country/occupation. \n",
    "- Rest of the event related information can be used to generate the fact data for any further analytics on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visa code and types :\n",
    "- This dataset contains type of visa codes that are allowed with their respective descriptions.\n",
    "- This data can be combined with I94 immigration dataset to add the types of visa along with code and desc.\n",
    "- This can be a dimension for analysing the number and types of visa that have been issued/arrived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|code|visa_desc|\n",
      "+----+---------+\n",
      "|   1| Business|\n",
      "|   2| Pleasure|\n",
      "|   3|  Student|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visa_data_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mode of Arrival\n",
    "- Contains types of mode that can be used to enter the country.\n",
    "- Can be further combined with even data for any information related mode of arrival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|code|        mode|\n",
      "+----+------------+\n",
      "|   1|         Air|\n",
      "|   2|         Sea|\n",
      "|   3|        Land|\n",
      "|   9|Not reported|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mode_data_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List of Countries :\n",
    "- Dataset contains list of country codes along with names.\n",
    "- This data can be used to analyse I94 data related country of origin / residence of immigrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "|code|          name|\n",
      "+----+--------------+\n",
      "| 582|MEXICO Air Sea|\n",
      "| 236|   AFGHANISTAN|\n",
      "| 101|       ALBANIA|\n",
      "| 316|       ALGERIA|\n",
      "| 102|       ANDORRA|\n",
      "+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_data_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List of states :\n",
    "- Lists of states codes and name of valid states, can be combined with city demographics data to create on dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|code|     state|\n",
      "+----+----------+\n",
      "|  AL|   ALABAMA|\n",
      "|  AK|    ALASKA|\n",
      "|  AZ|   ARIZONA|\n",
      "|  AR|  ARKANSAS|\n",
      "|  CA|CALIFORNIA|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "states_data_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ports :\n",
    "- List of ports that have all valid and invalid ports. This can be used to make sure I94 event data has only valid codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|code|               ports|\n",
      "+----+--------------------+\n",
      "| ALC|ALCAN, AK        ...|\n",
      "| ANC|ANCHORAGE, AK    ...|\n",
      "| BAR|BAKER AAF - BAKER...|\n",
      "| DAC|DALTONS CACHE, AK...|\n",
      "| PIZ|DEW STATION PT LA...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ports_data_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List of airports :\n",
    "- Name of airports along with data like type of airport, iata_code, elevation, co-ordinates etc.\n",
    "- This joined with event data would get the information for specific airport "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_data_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### City Demographic information\n",
    "- Dataset contains city demographic information. \n",
    "- City and State related data can be combined and put into one dimension for further analysis with event data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_data_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and Transform Data\n",
    "\n",
    "Generic steps to be performed on each dataset :\n",
    "- Find nulls and replace with proper code, date or 'UNKNOWN' if no values is known at this moment\n",
    "- Give proper alias to the columns\n",
    "- Select only required columns in intermediate df(s), eliminate the ones that are not needed\n",
    "- Typecast the columns whenever required\n",
    "\n",
    "\n",
    "<br>Here are the clean and transform steps performed on each of the dataset to create dim and fact tables\n",
    "##### UDF to remove the '0' from leading position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_padding = F.udf(lambda x: x.lstrip('0') if x else '0', StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### visa dim : \n",
    "- change the data type of visa code to integer\n",
    "- assign proper alias to columns\n",
    "- join with immigiration event data to also get the visa type \n",
    "- create a dim table / df for visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------+\n",
      "|code|type|    desc|\n",
      "+----+----+--------+\n",
      "|   1|  I1|Business|\n",
      "|   1|  B1|Business|\n",
      "|   1|  E1|Business|\n",
      "|   1|  WB|Business|\n",
      "|   1|  E2|Business|\n",
      "+----+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imm_df_stg = (\n",
    "    immigration_data_df\n",
    "    .select(\n",
    "        F.col(\"i94visa\").cast(IntegerType()).alias(\"code\"),\n",
    "        F.col(\"visatype\").alias(\"visa_type\")\n",
    "    )\n",
    ").distinct()\n",
    "\n",
    "\n",
    "visa_dim = (\n",
    "    visa_data_df\n",
    "    .join(imm_df_stg, visa_data_df.code == imm_df_stg.code, how = 'left')\n",
    "    .select(\n",
    "        visa_data_df.code.cast(IntegerType()).alias(\"code\"),\n",
    "        imm_df_stg.visa_type.alias(\"type\"),\n",
    "        visa_data_df.visa_desc.alias(\"desc\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "visa_dim.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mode dim:\n",
    "- assign appropiate datatype to code\n",
    "- assign meaningful aliases to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|code|        mode|\n",
      "+----+------------+\n",
      "|   1|         Air|\n",
      "|   2|         Sea|\n",
      "|   3|        Land|\n",
      "|   9|Not reported|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mode_dim = (\n",
    "    mode_data_df\n",
    "    .select(\n",
    "        F.col(\"code\").cast(IntegerType()).alias(\"code\"), \n",
    "        F.col(\"mode\")\n",
    "    )\n",
    ")\n",
    "mode_dim.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### country dim:\n",
    "- Select only valid codes to create this data set eliminate the invalid code.\n",
    "- Typecast code to have integer data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "|code|          name|\n",
      "+----+--------------+\n",
      "| 582|MEXICO Air Sea|\n",
      "| 236|   AFGHANISTAN|\n",
      "| 101|       ALBANIA|\n",
      "| 316|       ALGERIA|\n",
      "| 102|       ANDORRA|\n",
      "+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_dim = (\n",
    "    country_data_df\n",
    "    .select(\n",
    "        F.col(\"code\").cast(IntegerType()), \n",
    "        F.col(\"name\"))\n",
    "    .filter(~F.col(\"name\").contains(\"No Country Code\"))\n",
    "    .filter(~F.col(\"name\").contains(\"INVALID\"))\n",
    ")\n",
    "country_dim.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ports dim:\n",
    "- Select only valid codes to create ports dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|code|               ports|\n",
      "+----+--------------------+\n",
      "| ALC|ALCAN, AK        ...|\n",
      "| ANC|ANCHORAGE, AK    ...|\n",
      "| BAR|BAKER AAF - BAKER...|\n",
      "| DAC|DALTONS CACHE, AK...|\n",
      "| PIZ|DEW STATION PT LA...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ports_dim = (\n",
    "    ports_data_df\n",
    "    .filter(~F.col(\"ports\").contains(\"No PORT Code\"))\n",
    ")\n",
    "\n",
    "ports_dim.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### airports dim:\n",
    "- Select records that have proper IATA Code\n",
    "- Split coordinates to separate columns latitude and longitude\n",
    "- Typecast the columns whereever needed\n",
    "- Remove any duplicate records\n",
    "- Removing any leading '0' from local codes\n",
    "- Replace nulls with proper code : 'UNKNOWN'\n",
    "- Assign appropiate alias where necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+-----------+----------+------------+--------+---------+----------+------------------+-----------------+\n",
      "|ident|         type|                name|elevation_ft|iso_country|iso_region|municipality|gps_code|iata_code|local_code|          latitude|        longitude|\n",
      "+-----+-------------+--------------------+------------+-----------+----------+------------+--------+---------+----------+------------------+-----------------+\n",
      "|  AQY|small_airport|    Girdwood Airport|         150|         US|     US-AK|    Girdwood|     UNK|      AQY|       AQY|       -149.126007|        60.966099|\n",
      "| AYHH|small_airport|    Honinabi Airport|         452|         PG|    PG-WPD|    Honinabi|    AYHH|      HNN|       HBI|          142.1771|         -16.2457|\n",
      "| BIDV|small_airport| DjÃºpivogur Airport|           9|         IS|      IS-7| DjÃºpivogur|    BIDV|      DJU|       UNK|-14.28279972076416|64.64420318603516|\n",
      "| CAM3|small_airport|      Duncan Airport|         300|         CA|     CA-BC|      Duncan|    CAM3|      DUQ|      CAM3|    -123.709702492|    48.7545336204|\n",
      "| CEQ5|small_airport|Grande Cache Airport|        4117|         CA|     CA-AB|Grande Cache|    CEQ5|      YGC|      CEQ5|  -118.87400054932|  53.916900634766|\n",
      "+-----+-------------+--------------------+------------+-----------+----------+------------+--------+---------+----------+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_dim = (\n",
    "    airport_data_df\n",
    "    .select(airport_data_df.columns)\n",
    "    .withColumn(\"local_code\", F.when(F.col(\"local_code\").isNull(), 'UNK').otherwise(F.col(\"local_code\")).alias(\"local_code\"))\n",
    "    .withColumn(\"local_code\", remove_padding(\"local_code\"))\n",
    "    .withColumn(\"latitude\", F.split(\"coordinates\",',')[0])\n",
    "    .withColumn(\"longitude\", F.split(\"coordinates\",',')[1])\n",
    "    .select(\n",
    "        F.col(\"ident\"), \n",
    "        F.col(\"type\"), \n",
    "        F.col(\"name\"),\n",
    "        F.col(\"elevation_ft\"), \n",
    "        F.col(\"iso_country\"),\n",
    "        F.when(F.col(\"iso_region\").isNull(), 'UNK').otherwise(F.col(\"iso_region\")).alias(\"iso_region\"),\n",
    "        F.when(F.col(\"municipality\").isNull(), 'UNK').otherwise(F.col(\"municipality\")).alias(\"municipality\"),\n",
    "        F.when(F.col(\"gps_code\").isNull(), 'UNK').otherwise(F.col(\"gps_code\")).alias(\"gps_code\"), \n",
    "        F.col(\"iata_code\"), \n",
    "        F.col(\"local_code\"),\n",
    "        F.col(\"latitude\").cast(DoubleType()), \n",
    "        F.col(\"longitude\").cast(DoubleType())\n",
    "    )\n",
    "    .where(\n",
    "        F.col(\"iata_code\").isNotNull())\n",
    "    .distinct()\n",
    "\n",
    ")\n",
    "\n",
    "airports_dim.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### states dim (states + city demographic data):\n",
    "- Get the state data from states df and city data from city df and combine those to create one dimension that has state_code, state_name, list of cities in states, their respective population\n",
    "- Assign appropiate alias where necessary\n",
    "- Remove duplicates from states and city df before joining these two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------+\n",
      "|      city|state_code|total_city_pop|\n",
      "+----------+----------+--------------+\n",
      "|   Suffolk|        VA|         88161|\n",
      "|Enterprise|        NV|        135474|\n",
      "+----------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+---------+\n",
      "|code|    state|\n",
      "+----+---------+\n",
      "|  TN|TENNESSEE|\n",
      "|  KS|   KANSAS|\n",
      "+----+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+----------+------------+--------------+\n",
      "|state_code|state_name|        city|total_city_pop|\n",
      "+----------+----------+------------+--------------+\n",
      "|        TN| TENNESSEE|Murfreesboro|        126121|\n",
      "|        TN| TENNESSEE|Johnson City|         65369|\n",
      "|        TN| TENNESSEE|     Memphis|        655760|\n",
      "|        TN| TENNESSEE| Chattanooga|        176597|\n",
      "|        TN| TENNESSEE|     Jackson|         66980|\n",
      "+----------+----------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_df_ = (\n",
    "    city_data_df\n",
    "    .select(\n",
    "        F.col(\"city\"), \n",
    "        F.col(\"State Code\").alias(\"state_code\"),\n",
    "        F.col(\"Total Population\").alias(\"total_city_pop\")\n",
    "    )\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "city_df_.show(2)\n",
    "\n",
    "\n",
    "\n",
    "states_df_ = (\n",
    "    states_data_df\n",
    "    .select(\n",
    "        F.col(\"code\"), \n",
    "        F.col(\"state\")\n",
    "    )\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "states_df_.show(2)\n",
    "\n",
    "\n",
    "\n",
    "states_dim = (\n",
    "    states_df_\n",
    "    .join(city_df_, states_df_.code == city_df_.state_code,how='left')\n",
    "    .select(\n",
    "        states_df_.code.alias(\"state_code\"),\n",
    "        states_df_.state.alias(\"state_name\"),\n",
    "        city_df_.city,\n",
    "        city_df_.total_city_pop\n",
    "    )\n",
    ")\n",
    "\n",
    "states_dim.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### non immigrant dim:\n",
    "- This data is retrived from I94 immigration event data\n",
    "- Data that is relevent to immigrant like cicid, birth year, gender, occup, country of residence that is not going to change frequently can be put in separate dimension for analysis on immigrants related data\n",
    "- Retrive the above columns, typecast and assign aliases wherever necessary\n",
    "- Replace nulls with code : UNKNOWN or UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+------+-----+--------------+\n",
      "|  cicid|birth_year|age_in_years|gender|occup|country_of_res|\n",
      "+-------+----------+------------+------+-----+--------------+\n",
      "|5748517|      1976|          40|     F|  UNK|           438|\n",
      "|5748518|      1984|          32|     F|  UNK|           438|\n",
      "|5748519|      1987|          29|     M|  UNK|           438|\n",
      "|5748520|      1987|          29|     F|  UNK|           438|\n",
      "|5748521|      1988|          28|     M|  UNK|           438|\n",
      "+-------+----------+------------+------+-----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "non_imm_dim = (\n",
    "    immigration_data_df\n",
    "    .select(\n",
    "        F.col(\"cicid\").cast(IntegerType()),\n",
    "        F.col(\"biryear\").alias(\"birth_year\").cast(IntegerType()),\n",
    "        F.col(\"i94bir\").cast(IntegerType()).alias(\"age_in_years\"),\n",
    "        F.col(\"gender\"),\n",
    "        F.when(F.col(\"occup\").isNull(),'UNK').otherwise(F.col(\"occup\")).alias(\"occup\"),\n",
    "        F.col(\"i94res\").alias(\"country_of_res\").cast(IntegerType())\n",
    "    )\n",
    ")\n",
    "\n",
    "non_imm_dim.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### date dim:\n",
    "- This dimension would be useful when any analysis needs to be performed w.r.t specific date, month, year or quarter\n",
    "- There are other ways to create time dim but for this specific use case get all the arrival and departure dates from i94 immigrantion data\n",
    "- Get a union of both the data sets, eliminate duplicates\n",
    "- Extract year, month, day, datekey, quarter, day_of_month, day_of_week, week_of_year from the date and assign to appropiate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+--------+-------+------------+-----------+------------+\n",
      "|      date|year|month|day| datekey|quarter|day_of_month|day_of_week|week_of_year|\n",
      "+----------+----+-----+---+--------+-------+------------+-----------+------------+\n",
      "|2001-07-20|2001|    7| 20|20010720|      3|          20|          6|          29|\n",
      "|2012-04-12|2012|    4| 12|20120412|      2|          12|          5|          15|\n",
      "|2012-04-14|2012|    4| 14|20120414|      2|          14|          7|          15|\n",
      "|2014-04-22|2014|    4| 22|20140422|      2|          22|          3|          17|\n",
      "|2014-04-24|2014|    4| 24|20140424|      2|          24|          5|          17|\n",
      "+----------+----+-----+---+--------+-------+------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the arrival and departure dates from Immigration dataset, combine and remove dups\n",
    "\n",
    "time_df_a = (\n",
    "    immigration_data_df\n",
    "    .select(\n",
    "        F.col(\"arrdate\").alias(\"date\"))\n",
    "    .where(F.col(\"arrdate\").isNotNull())\n",
    ")\n",
    "\n",
    "time_df_d = (\n",
    "    immigration_data_df\n",
    "    .select(\n",
    "        F.col(\"depdate\").alias(\"date\"))\n",
    "    .where(F.col(\"depdate\").isNotNull())\n",
    ")\n",
    "\n",
    "time_df_ = (\n",
    "    time_df_a\n",
    "    .union(time_df_d)\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "\n",
    "# Transform the date in mm/dd/yyyy format\n",
    "\n",
    "time_df_ = (\n",
    "    time_df_\n",
    "    .select(\n",
    "        F.col(\"date\").cast(IntegerType())\n",
    "    )\n",
    ")\n",
    "\n",
    "time_df_ = (\n",
    "    time_df_\n",
    "    .withColumn(\"sas_date\", F.to_date(F.lit(\"01/01/1960\"), \"MM/dd/yyyy\"))\n",
    ")\n",
    "\n",
    "time_df_ = (\n",
    "    time_df_\n",
    "    .withColumn(\"date\", F.expr(\"date_add(sas_date, date)\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Derive the required columns from date\n",
    "\n",
    "date_dim = (\n",
    "    time_df_\n",
    "    .select(\"date\")\n",
    "    .withColumn(\"year\", F.year(\"date\"))\n",
    "    .withColumn(\"month\", F.month(\"date\"))\n",
    "    .withColumn(\"day\", F.dayofmonth(\"date\"))\n",
    "    .withColumn(\"datekey\", F.date_format(F.col(\"date\"), \"yyyyMMdd\"))\n",
    "    .withColumn(\"quarter\", F.quarter(\"date\"))\n",
    "    .withColumn(\"day_of_month\", F.dayofmonth(\"date\"))\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"date\"))\n",
    "    .withColumn(\"week_of_year\", F.weekofyear(\"date\"))\n",
    "    .sort(\"datekey\")\n",
    ")\n",
    "\n",
    "date_dim.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### immigration dataset - fact:\n",
    "- Transform arrival and departure date to mm/dd/yyyy format\n",
    "- Remove leading '0' from flight no\n",
    "- Rename the columns to more readable names\n",
    "- Assign UNKNOWN to nulls values if the value is not known\n",
    "- Where ever the mode of travel is not 'Air' assign 'Not Applicable' to airline Column\n",
    "- Cast col adm_num to long type and cicid, country_of_origin, arr_mode_code, visa_code to Integer\n",
    "- Get the distinct ports from ports dim and local code from airport dim and create an intermediate data frame that has union of list of codes from both airport and port dims\n",
    "- semi left join following dims to ensure the immigration fact table has valid data (foriegn key dependency)\n",
    "\t- mode_dim on mode_code\n",
    "\t- country_dim on country code\n",
    "\t- visa_dim on visa code\n",
    "\t- intermediate ports dim (airports_dim local codes + port from ports_dim) on port code\n",
    "\t- states_dim on state_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------------+--------+-------------+--------------+----------+----------+---------+--------+------------------+-------+------+----------+\n",
      "|    adm_num| cic_id|country_of_origin|arr_port|arr_mode_code|arr_state_code|  arr_date|  dep_date|visa_code|visatype|visa_issuing_state|airline|flt_no|date_added|\n",
      "+-----------+-------+-----------------+--------+-------------+--------------+----------+----------+---------+--------+------------------+-------+------+----------+\n",
      "|94166868730|3867886|              135|     NYC|            1|            NY|2016-04-21|2016-08-28|        1|      B1|               LND|     BA|   113|2016-01-21|\n",
      "|95002496530|5749447|              251|     NEW|            1|            PA|2016-04-30|2016-05-05|        1|      B1|               TLV|     UA|    85|2016-01-30|\n",
      "|94965421030|5756497|              260|     NYC|            1|            OH|2016-04-30|2016-05-11|        1|      B1|               MNL|     KE|    81|2016-01-30|\n",
      "|94954405430|5758677|              263|     CHI|            1|            PA|2016-04-30|2016-05-06|        1|      B1|               BNK|     NH|    12|2016-01-30|\n",
      "|94136361030|3883387|              209|     ATL|            1|            GA|2016-04-21|2016-05-13|        1|      E2|               KBO|     LH|   444|2016-01-21|\n",
      "+-----------+-------+-----------------+--------+-------------+--------------+----------+----------+---------+--------+------------------+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean and transform the columns\n",
    "df_imm_stg = (\n",
    "    immigration_data_df\n",
    "    .select(\n",
    "        immigration_data_df.columns)\n",
    "    .withColumn(\"arr_date\", F.col(\"arrdate\").cast(IntegerType()))\n",
    "    .withColumn(\"dep_date\", F.col(\"depdate\").cast(IntegerType()))\n",
    "    .withColumn(\"sas_date\", F.to_date(F.lit(\"01/01/1960\"), \"MM/dd/yyyy\"))\n",
    "    .withColumn(\"arr_date\", F.expr(\"date_add(sas_date, arr_date)\"))\n",
    "    .withColumn(\"dep_date\", F.expr(\"date_add(sas_date, dep_date)\"))\n",
    "    .withColumn(\"flt_no\", remove_padding(\"fltno\"))\n",
    "    .select(F.col(\"admnum\").cast(LongType()).alias(\"adm_num\"),\n",
    "            F.col(\"cicid\").cast(IntegerType()).alias(\"cic_id\"),\n",
    "            F.col(\"i94cit\").cast(IntegerType()).alias(\"country_of_origin\"),\n",
    "            F.col(\"i94port\").alias(\"arr_port\"),\n",
    "            F.col(\"i94mode\").cast(IntegerType()).alias(\"arr_mode_code\"),\n",
    "            F.when(F.col(\"i94addr\").isNull(), 'UNK').otherwise(F.col(\"i94addr\")).alias(\"arr_state_code\"),\n",
    "            F.col(\"arr_date\"),\n",
    "            F.when(F.col(\"dep_date\").isNull(), F.to_date(F.lit(\"12/31/9999\"), \"MM/dd/yyyy\")).otherwise(F.col(\"dep_date\")).alias(\"dep_date\"),\n",
    "            F.col(\"i94visa\").cast(IntegerType()).alias(\"visa_code\"),\n",
    "            F.col(\"visatype\"), \n",
    "            F.when(F.col(\"visapost\").isNull(), 'UNK').otherwise(F.col(\"visapost\")).alias(\"visa_issuing_state\"),\n",
    "            F.when((F.col(\"airline\").isNull() & (F.col(\"i94mode\").cast(IntegerType()) != 1)), 'Not Applicable').otherwise(F.col(\"airline\")).alias(\"airline\"),\n",
    "            F.col(\"flt_no\"),\n",
    "            F.to_date(F.lit(F.col(\"dtadfile\")),'yyyymmdd').alias(\"date_added\")\n",
    "           )\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# Create intermediate df to get the codes from both airport and ports dimension to be joined with immigration data in next step, \n",
    "# this will ensure we get only valid codes in immigration fact table\n",
    "df_ports_stg = (\n",
    "    ports_dim\n",
    "    .select(\n",
    "        F.col(\"code\"))\n",
    ")\n",
    "\n",
    "df_airports_stg = (\n",
    "    airports_dim\n",
    "    .select(\n",
    "        F.col(\"local_code\"))\n",
    ")\n",
    "\n",
    "df_ports_validation = (\n",
    "    df_ports_stg\n",
    "    .select('code')\n",
    "    .union(df_airports_stg\n",
    "           .select('local_code'))\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# join with rest of the dimension tables to filter out any invalid records that are not present in dimension data sets\n",
    "immigration_fact = (\n",
    "    df_imm_stg\n",
    "    .join(mode_dim, df_imm_stg.arr_mode_code == mode_dim.code, how=\"leftsemi\")\n",
    "    .join(country_dim, df_imm_stg.country_of_origin == country_dim.code, how=\"leftsemi\")\n",
    "    .join(df_ports_validation, df_ports_validation.code == df_imm_stg.arr_port, how=\"leftsemi\")\n",
    "    .join(visa_dim, visa_dim.code == df_imm_stg.visa_code, how=\"leftsemi\")\n",
    "    .join(states_dim, states_dim.state_code == df_imm_stg.arr_state_code, how=\"leftsemi\")\n",
    "    .select(df_imm_stg.columns)\n",
    ")\n",
    "\n",
    "immigration_fact.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The purpose of creating this section is to provide an overview of the data design for this specific project and how it is going to be categorized for analytics.\n",
    "\n",
    "Here are the dimensions that are identified from the immigration data:\n",
    "- Visa\n",
    "- Mode of travel\n",
    "- List of Countries\n",
    "- Port of entry:\n",
    "\t- Ports\n",
    "\t- Airports \n",
    "- States\n",
    "- Immigrant\n",
    "- Date\n",
    "\n",
    "Dataset that has log of each entry and exit of an immigrant (source for fact table):\n",
    "- I94 Data\n",
    "\n",
    "Following diagram shows the relationship between dimensions and fact, along with their respective primary and foreign keys.\n",
    "\n",
    "Schema to be stored in 1NF and would require minimal joins to fetch the information.\n",
    "\n",
    "\n",
    "\n",
    "<!-- ![alt text](images/data_model.jpeg) -->\n",
    "\n",
    "<img src=\"images/data_model.jpeg\" alt=\"conceptual DM\" width=\"1000\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "The data pipeline would have following steps:\n",
    "1) Read the source and load the data into intermediate dataframes, assuming that the data is stored in s3 and the s3 would be used as source.\n",
    "2) Clean the data and transform the columns/data in desired format.\n",
    "3) Load the data in s3 into a 'star schema' which is also a 'schema on read' . This schema can be further analyzed using spark/panda for analytics or can be further transformed and loaded into olap cubes in a redshift dataware house.\n",
    "\n",
    "The target schema will have data in parquet format with fact and date dimension partitioned by year and month.\n",
    "The fact table will have data that is present in dimension table to ensure the referential integrity although it's not enforced but the pipeline is design to ensure this.\n",
    "\n",
    "\n",
    "There are also additional steps to be included in pipeline pertaining to project setup and cluster creation and other validations those also needs to be included as part of this pipeline to ensure all the pieces work together end to end to minimize the errors and the steps in pipeline would be orchestrated with the help of **Apache Airflow**.<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### ETL DAG:\n",
    "\n",
    "**upload_bootstrap_task_to_s3** :  Shell script to be uploaded in s3 which runs during the creation of EMR cluster which setup the project workspace and also install any requirements/dependencies for python modules to load and run properly.\n",
    "\n",
    "**create_project_package** : Bash command to package the project into a tar file.\n",
    "\n",
    "**upload_to_s3** :  Upload the project tar to s3 which is used by the bootstrap script to setup project space on EMR. Though zip files can be accessed from s3 directly while invoking the spark-submit but this step ensures there are no import errors when the spark runs.\n",
    "\n",
    "**run_etl_spark** : Invokes etl.py from project /scripts directory which includes following steps:\n",
    "1) Creates spark session \n",
    "2) Extract the data from s3 into spark data frames\n",
    "3) Clean and transform the data into dim and fact tables\n",
    "4) Load the data back in s3 in parquet format, creating a schema on read\n",
    "\n",
    "**watch_step_etl** : This step ensures the validation wont's run until the output is uploaded in s3.\n",
    "\n",
    "**run_data_validation_spark** : This steps performs validation on the output schema/tables.\n",
    "\n",
    "**watch_step_validate** : This step ensures the cluster won’t terminate until spark job is completed.\n",
    " \n",
    "**terminate_cluster** : Issue terminate cluster when the spark job is completed.\n",
    "\n",
    "\n",
    "<img src=\"images/airflow_pipeline.png\" alt=\"pipeline\" width=\"1500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "The data pipeline as mentioned above is created in Airflow and is manual for now since this is just a sample project to show the creation of ETL end to end.<br>\n",
    "Here is what project structure looks like : \n",
    "- **CapstoneProject** : contains ETL code in python\n",
    "- **dags** : contains airflow dags\n",
    "- **logs** : contains airflow workflow logs\n",
    "- **plugins**: contains airflow helpers and operators\n",
    "\n",
    "<img src=\"images/project.png\" alt=\"etl\" width=\"600\"/>\n",
    "\n",
    "\n",
    "<br>Here is what ETL code sturture in **CapstoneProject** looks like :\n",
    "\n",
    "<img src=\"images/ETL.png\" alt=\"etl\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>To start the apache airflow on local host intiate docker compose up in detached mode. \n",
    "\n",
    "This will start  all the services defined in a docker-compose.yml along with the latest code in project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker Compose is now in the Docker CLI, try `docker compose up`\n",
      "\n",
      "Creating network \"docker_default\" with the default driver\n",
      "Creating docker_postgres_1 ... \n",
      "Creating docker_redis_1    ... \n",
      "\u001b[1Bting docker_redis_1    ... \u001b[32mdone\u001b[0m\u001b[1A\u001b[2KCreating docker_flower_1   ... \n",
      "Creating docker_airflow-init_1 ... \n",
      "Creating docker_airflow-worker_1 ... \n",
      "Creating docker_airflow-scheduler_1 ... \n",
      "Creating docker_airflow-webserver_1 ... \n",
      "\u001b[1Bting docker_airflow-webserver_1 ... \u001b[32mdone\u001b[0m\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K"
     ]
    }
   ],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Airflow dags can be initiated using airflow API(s) programitically. \n",
    "\n",
    "Here are the steps to run the dags:\n",
    "\n",
    "1) Check if the dag \"Spark_S3_ETL\" exists in airflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"dag_id\": \"Spark_S3_ETL\",\n",
      "  \"description\": \"Load and transform data S3 using spark and airflow\",\n",
      "  \"file_token\": \"Ii9vcHQvYWlyZmxvdy9kYWdzL2RhZy5weSI.cXPa1UbBIsJCgfCxsH65tlZvqB4\",\n",
      "  \"fileloc\": \"/opt/airflow/dags/dag.py\",\n",
      "  \"is_paused\": false,\n",
      "  \"is_subdag\": false,\n",
      "  \"owners\": [\n",
      "    \"CapstoneProject\"\n",
      "  ],\n",
      "  \"root_dag_id\": null,\n",
      "  \"schedule_interval\": {\n",
      "    \"__type\": \"CronExpression\",\n",
      "    \"value\": \"@once\"\n",
      "  },\n",
      "  \"tags\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl 'http://localhost:8080/api/v1/dags/Spark_S3_ETL' -H 'content-type: application/json' --user \"airflow:airflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> \n",
    "2) If dag runs are paused make sure to unpause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"dag_id\": \"Spark_S3_ETL\",\n",
      "  \"description\": \"Load and transform data S3 using spark and airflow\",\n",
      "  \"file_token\": \"Ii9vcHQvYWlyZmxvdy9kYWdzL2RhZy5weSI.iCswl9I_DJedNikdzjNIehRDrjs\",\n",
      "  \"fileloc\": \"/opt/airflow/dags/dag.py\",\n",
      "  \"is_paused\": false,\n",
      "  \"is_subdag\": false,\n",
      "  \"owners\": [\n",
      "    \"CapstoneProject\"\n",
      "  ],\n",
      "  \"root_dag_id\": null,\n",
      "  \"schedule_interval\": {\n",
      "    \"__type\": \"CronExpression\",\n",
      "    \"value\": \"@once\"\n",
      "  },\n",
      "  \"tags\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X PATCH -d '{\"is_paused\": false}' 'http://localhost:8080/api/v1/dags/Spark_S3_ETL?update_mask=is_paused' -H 'content-type: application/json' --user \"airflow:airflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "3) Intiate the dag run by the execution date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"conf\": {},\n",
      "  \"dag_id\": \"Spark_S3_ETL\",\n",
      "  \"dag_run_id\": \"manual__2021-07-09T13:21:00+00:00\",\n",
      "  \"end_date\": null,\n",
      "  \"execution_date\": \"2021-07-09T13:21:00+00:00\",\n",
      "  \"external_trigger\": true,\n",
      "  \"start_date\": \"2021-07-09T20:21:39.730742+00:00\",\n",
      "  \"state\": \"running\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST -d '{\"execution_date\": \"2021-07-09T13:21:00Z\", \"conf\": {}}' 'http://localhost:8080/api/v1/dags/Spark_S3_ETL/dagRuns' -H 'content-type: application/json' --user \"airflow:airflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "4) Check the status of dag execution to ensure it was a success and validate the data in s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"conf\": {},\n",
      "  \"dag_id\": \"Spark_S3_ETL\",\n",
      "  \"dag_run_id\": \"manual__2021-07-09T13:21:00+00:00\",\n",
      "  \"end_date\": \"2021-07-09T20:21:48.011080+00:00\",\n",
      "  \"execution_date\": \"2021-07-09T13:21:00+00:00\",\n",
      "  \"external_trigger\": true,\n",
      "  \"start_date\": \"2021-07-09T20:21:39.730742+00:00\",\n",
      "  \"state\": \"failed\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl 'http://localhost:8080/api/v1/dags/Spark_S3_ETL/dagRuns/manual__2021-07-09T13:21:00+00:00' -H 'content-type: application/json' --user \"airflow:airflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Stop the docker down when the job is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping docker_airflow-webserver_1 ... \n",
      "Stopping docker_airflow-scheduler_1 ... \n",
      "Stopping docker_airflow-worker_1    ... \n",
      "Stopping docker_flower_1            ... \n",
      "Stopping docker_redis_1             ... \n",
      "Stopping docker_postgres_1          ... \n",
      "\u001b[2Bping docker_redis_1             ... \u001b[32mdone\u001b[0m\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2KRemoving docker_airflow-webserver_1 ... \n",
      "Removing docker_airflow-scheduler_1 ... \n",
      "Removing docker_airflow-worker_1    ... \n",
      "Removing docker_airflow-init_1      ... \n",
      "Removing docker_flower_1            ... \n",
      "Removing docker_redis_1             ... \n",
      "Removing docker_postgres_1          ... \n",
      "\u001b[7BRemoving network docker_default ... \u001b[32mdone\u001b[0m\u001b[3A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\n"
     ]
    }
   ],
   "source": [
    "!docker-compose down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Following data quality checks are performed after the ETL pipe line is finished. The step has run manually here to show the output however it is part of data pipeline and is orchestrated by APACHE AIRFLOW. Please check the pipeline image in previous section for more information.\n",
    "\n",
    "STEPS:\n",
    "- Data Validation 1:\n",
    "    - A check is performed on all the relevent dimensions to ensure the referential integrity. To make sure respective FK(s) in fact table are valid and are present in dimension table.\n",
    "- Data Validation 2:\n",
    "    - Check on primary keys to ensure there are no nulls.\n",
    "- Data Validation 3:\n",
    "    - Dimension table are queried to make sure the count after load is not 0.\n",
    "\n",
    "SCRIPTS INVOLVED:\n",
    "- validator.py : driver script that invokes data_validator.py\n",
    "- data_validator.py : script that invokes spark.py and load_schema.py to create spark session and read schema respectively. Also run all the checks mentioned above\n",
    "- load_schema.py: script that reads the data from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Integrity Validation Completed : visa_dim\n",
      "Data Integrity Validation Completed : mode_dim\n",
      "Data Integrity Validation Completed : country_dim\n",
      "Data Integrity Validation Completed : ports and airports dim\n",
      "Data Integrity Validation Completed : states_dim\n",
      "Not Null Validation Passed : visa_dim\n",
      "Not Null Validation Passed : mode_dim\n",
      "Not Null Validation Passed : country_dim\n",
      "Not Null Validation Passed : ports_dim\n",
      "Not Null Validation Passed : airports_dim\n",
      "Not Null Validation Passed : states_dim\n",
      "Not Null Validation Passed : non_imm_dim\n",
      "Not Null Validation Passed : adm_num\n",
      "Check Rows Validation Passed : visa_dim\n",
      "Check Rows Validation Passed : mode_dim\n",
      "Check Rows Validation Passed : country_dim\n",
      "Check Rows Validation Passed : ports_dim\n",
      "Check Rows Validation Passed : airports_dim\n",
      "Check Rows Validation Passed : states_dim\n",
      "Check Rows Validation Passed : non_imm_dim\n"
     ]
    }
   ],
   "source": [
    "%run CapstoneProject/validator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "**non_imm_dim**: Dimension table that stored immigrant’s information<br>\n",
    "*cicid* : client id<br>\n",
    "*birth_year* : year of birth<br>\n",
    "*age_in_years* : age of immigrant in years<br>\n",
    "*gender* : gender of immigrant<br>\n",
    "*occup* : occupation of immigrant<br>\n",
    "*country_of_res* : Immigrant’s country of residence<br>\n",
    "\n",
    "**date_dim**: Date dimension that would help with date wise reporting<br>\n",
    "*date* : date of arr/dep<br>\n",
    "*year* : year of arr/dep<br>\n",
    "*month* : month of arr/dep<br>\n",
    "*day* : day of arr/dep<br>\n",
    "*datekey* : date in integer format<br>\n",
    "*quarter* : quarter of arr/dep<br>\n",
    "*day_of_month* : day of month of arr/dep<br>\n",
    "*day_of_week* : day of week of arr/dep<br>\n",
    "*week_of_year* : week of the year of arr/dep<br>\n",
    "\n",
    "**mode_dim** : This dimension contains mode of travel along with the code<br>\n",
    "*code* : number associated to mode of travel<br>\n",
    "*mode* : mode description<br>\n",
    "\n",
    "**states_dim** : Contains information related to states<br>\n",
    "*state_code* : unique code associated to state<br>\n",
    "*state_name* : name of the state<br>\n",
    "*city* :  city name <br>\n",
    "*total_city_pop* : Population of city<br>\n",
    "\n",
    "\n",
    "**airports_dim** : This dimension contains information related to airports<br>\n",
    "*ident* : Unique code to identify airport<br>\n",
    "*type* : size of airport<br>\n",
    "*name* : name of the airport<br>\n",
    "*elevation_ft* : elevation in feet<br>\n",
    "*iso_county* : country (ISO-2)<br>\n",
    "*iso_region* : region (ISO-2)<br>\n",
    "*municipality* :  municipality<br>\n",
    "*gps_code* : gps code<br>\n",
    "*iata_code* : location identifier<br>\n",
    "*local_code* : local code for the airport<br>\n",
    "*latitude* : latitude<br>\n",
    "*longitude* : longitude<br>\n",
    "\n",
    "**country_dim** : This dimension contains list of countries<br>\n",
    "*code* : unique for the country<br>\n",
    "*name* : name of country<br>\n",
    "\n",
    "**ports_dim** : This dimension contains port related information, to identify port of entry<br>\n",
    "*code* : unique code associated to port<br>\n",
    "*ports* : port name<br>\n",
    "\n",
    "**visa_dim** : Contains information related to visa<br>\n",
    "*code* :  Unique visa code<br>\n",
    "*type* : Type of visa<br>\n",
    "*desc* : Visa desc<br>\n",
    "\n",
    "**immigration_fact** : This table contains the entry and exit records<br>\n",
    "*adm_num* : admission record number<br>\n",
    "*cicid* : client id<br>\n",
    "*country_of_origin* : country of origin of non-immigrant<br>\n",
    "*arr_port* : arrival port of immigrant<br>\n",
    "*arr_mode_code* : arrival mode <br>\n",
    "*arr_state_code* : state where immigrant arrived<br>\n",
    "*arr_date* : date of arrival<br>\n",
    "*dep_date* : departure date<br>\n",
    "*visa_code* : visa that was issued to immigrant<br>\n",
    "*visatype* : visa type<br>\n",
    "*visa_issuing_state* : Visa issues state<br>\n",
    "*airline* : airline used for travel<br>\n",
    "*flt_no* : flight number <br>\n",
    "*date_added* : date record was added in system<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Project Write Up\n",
    "<!-- * Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1) The scope of this project is to have a simple ETL pipeline which reads the data from data files transform and create star schema in s3. This schema can be accessed by pandas or spark or other technologies to be processed further for analytics.The data stored in s3 is structured, this pipeline can be also be modifed to stored semi structure data and to have raw, semi structured and strutured data to create data lakes. It's versatile due to the technologies used in the project.\n",
    "\n",
    "    - **SPARK** : Since we are dealing with huge data here and reading the data from data files, spark was a clear choice here as it support wide range of data formats and also the in memory processing is much faster. With pyspark it's easy to use dataframe APIs for processing large chunk of structured or semi-structured data.\n",
    "    - **AWS EMR** : With spark we just need the cluster for it's processing power and for that EMR was an easy choice. EMR clusters are easy to setup, can scale up and down depending on the workload and it has very good integration with APACHE AIRFLOW which is used as orchestrator for this project.\n",
    "    - **AWS S3** : S3 is used for creating schema on read. It's cheap storage, easy to setup and easily accessible from EMR clusters.\n",
    "    - **APACHE AIRFLOW** : It is an open source tool for orchestrating complex data pipelines. It already has wide range of operators and apis that minimize the need of coding or creating custom operator. In this project we used s3 hooks and EMR operators. since python is the main lauguage used to create pipeline and apache airflow is written in python it was easy to integrate with this project. It also has CLI and API support to trigger the pipeline programatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The data should be updated on **montly basis**. While analyzing the data I felt that montly load would make more sense then daily. The data size would be manageable and it's not business critical and I am assuming that it would be used mostly to check monthly, quatertly or yearly metric. Hence, it makes more sense to have the load when complete montly's data arrive on 1st of every month at midnight. We can use Apache Airflow to ingest the data. Also the fact table is paritioned by year and month, it would be easy to create new parition for each month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "3) Spark is used to populate s3 schema, which make it easier to deal with the large datasets even when the data is **increased by 100x**. The EMR clusters can scale up and down depending on the workload provided autoscale is configured.\n",
    "\n",
    "    Spark also makes it easy and efficient to read the schema from s3 to perform any further analysis on data. The Data can be read and loaded into dataframes, can be used to generate the intermediate data cubes or it can be further transformed and loaded into RDS.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) With conventional databases as the traffic grows, bandwidth utilization increases hence this add the need to upgrade database instances to instance types with more bandwidth. This adds downtime and other setbacks that can hurt the business if data is business critical.<br>\n",
    "    **S3 is used as database / storage** option for final dataset which is both **scalable and distributed** can easily achieve thousands of transactions per second in request performance when uploading and retrieving the data from storage. Data stored in S3 are widely accessible through RESTful APIs. It can serve as a central static asset repository that knows to partition storage automatically to increase performance as the number of requests increases over time. <br>\n",
    "    Hence, the data needed to be accessed by 100+ people won't be a problem with s3 object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "5) Airflow is used as an orchestrator and jobs can be **scheduled** in airflow to run the pipeline whenever needed. The dag for this pipeline is not scheduled and is externally triggered. However, using Airflow it can be sheduled to run once a @month at midnight of 1st day for the month. It would require following changes in DAG :\n",
    ">      with DAG('Spark_S3_ETL', \n",
    ">          default_args=default_args, \n",
    ">          description='Load and transform data S3 using spark and airflow', \n",
    ">          schedule_interval='@montly' \n",
    ">        ) as dag:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Step 6: Query on final dataset\n",
    "Here are examples of some of the use cases we mentioned in first section. This model has 7 dimensions visa, mode, country, ports, airports, states, date, non_imm. We can used these descriptive attributes to generate multiple metrics with the help of data in fact table. Here are few exmaples of information retrived from final dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa_dim = spark.read.parquet(\"s3a://capstoneprojectsource/immigration_info/visa_dim.parquet\")\n",
    "mode_dim =  spark.read.parquet(\"s3a://capstoneprojectsource/immigration_info/mode_dim.parquet\")\n",
    "ports_dim = spark.read.parquet(\"s3a://capstoneprojectsource/immigration_info/ports_dim.parquet\")\n",
    "airports_dim = spark.read.parquet(\"s3a://capstoneprojectsource/immigration_info/airports_dim.parquet\")\n",
    "date_dim = spark.read.parquet(\"s3a://capstoneprojectsource/immigration_info/date_dim.parquet\")\n",
    "immigration_fact = spark.read.parquet(\"s3a://capstoneprojectsource/immigration_info/immigration_fact.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### 1) How many visas were issued overall or over a period of time? In the month of April, 2016 (dim: visa, dim: date, fact: immigration records)\n",
    "\n",
    "##### a) No. of visa issued - visa category wise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|visa_code|  count|\n",
      "+---------+-------+\n",
      "|        1| 430501|\n",
      "|        3|  35535|\n",
      "|        2|2085134|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_visa = (\n",
    "    immigration_fact\n",
    "    .join(visa_dim, immigration_fact.visa_code==visa_dim.code, how=\"leftsemi\")\n",
    "    .join(date_dim, immigration_fact.arr_date == date_dim.date, how=\"leftsemi\")\n",
    "    .select(immigration_fact.columns)\n",
    "    .where((immigration_fact.month==4) & (immigration_fact.year==2016))\n",
    ")\n",
    "\n",
    "df_visa_cat = df_visa.groupBy(\"visa_code\").agg(F.count(\"*\").alias(\"count\"))\n",
    "df_visa_cat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Total no. of visas issued \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(count)=2551170)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_visa_cat.select(\"count\").groupBy().sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### 2) What mode immigrants used the most to travel ? (dim: mode_dim, fact: immigration_fact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|        mode|No_of_entries|\n",
      "+------------+-------------+\n",
      "|         Air|      2497553|\n",
      "|        Land|        42370|\n",
      "|         Sea|         6045|\n",
      "|Not reported|         5202|\n",
      "+------------+-------------+\n",
      "\n",
      "Most Frequest mode of Travel\n",
      "+----+-------------+\n",
      "|mode|No_of_entries|\n",
      "+----+-------------+\n",
      "| Air|      2497553|\n",
      "+----+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mode = (\n",
    "    immigration_fact\n",
    "    .join(mode_dim, immigration_fact[\"arr_mode_code\"] == mode_dim[\"code\"])\n",
    "    .select(immigration_fact.arr_mode_code, mode_dim.mode)\n",
    "    .groupBy(mode_dim.mode)\n",
    "    .agg(F.count(\"arr_mode_code\").alias(\"No_of_entries\"))\n",
    "    .orderBy(F.col(\"No_of_entries\").desc())\n",
    ")\n",
    "\n",
    "df_mode.show()\n",
    "\n",
    "print(\"Most Frequest mode of Travel\")\n",
    "df_mode.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### 3) Which port received most no. of non-immigrants during a period of time? (dim: state, dim: airport, dim:date, fact immigration records)\n",
    "Data for 14th week of year (in April, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Port with most no. of entries in the 14th week of year\n",
      "+------------+-------------+\n",
      "|   port_name|no_of_entries|\n",
      "+------------+-------------+\n",
      "|NEW YORK, NY|        96527|\n",
      "+------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_ports = (ports_dim\n",
    "             .select(F.col(\"code\"), F.trim(F.col(\"ports\")).alias(\"name\"))\n",
    "             .unionAll(airports_dim.select(F.col(\"local_code\").alias(\"code\"), F.col(\"name\")))\n",
    "            ).distinct()\n",
    "df_ports = (\n",
    "    immigration_fact\n",
    "    .join(all_ports, immigration_fact[\"arr_port\"] == all_ports[\"code\"])\n",
    "    .join(date_dim, immigration_fact[\"arr_date\"] == date_dim[\"date\"])\n",
    "    .select(all_ports.name.alias(\"port_name\"), date_dim.week_of_year)\n",
    "    .where(date_dim.week_of_year == 14)\n",
    "    .groupBy(\"port_name\")\n",
    "    .agg(F.count(\"port_name\").alias(\"no_of_entries\"))\n",
    "    .orderBy(F.col(\"no_of_entries\").desc())\n",
    ")\n",
    "\n",
    "print(\"Port with most no. of entries in the 14th week of year\")\n",
    "df_ports.show(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
